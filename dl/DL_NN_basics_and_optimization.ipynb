{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network basics\n",
    "- A linear regression model predicts outcome by doing some of parts(inputs), and doesn't consider interactions between the parts (eg. no. of bank transactions from age, retirement status)\n",
    "- A Neural Network captures interactions between the parts very well.\n",
    "- Deep Learning uses specially powerful neural networks, and performs exceedingly well on prediction problems.\n",
    "- Their ability to capture extremely complex interactions allows them to perform well on texts, videos, images, audios, source codes, etc.\n",
    "- The intermediate nodes in hidden layers are used to calculate functions between variables (that accounts for their interactions, and use them to predict the outcome.\n",
    "- Each node in hidden layer represents an aggregation of info from input, and contribute to model's ability to capture interactions.\n",
    "- **!** The more nodes we have, the more interactions we capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "- A NN uses data to make predictions through 'forward propagation'\n",
    "- Each line(connection) coming out of a node has a **weight**, that indiciates how strongly that node effects the value of the hidden node at the end of it.\n",
    "- These weights are the parameters we train or change when we fit our neural network to data.\n",
    "- Multiply-Add process (dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/forward_prop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- For NN's to achieve their max predictive power, we use activation functions in hidden layers.\n",
    "- Allows model to capture non-linearities\n",
    "- Just Multiply-Add process is not enough\n",
    "- If the relationships in data aren't straight line relationships, we need activation functions that capture non-linearities.\n",
    "- Eg. Non-linearities capture pattens like how going from 0 to 1 child may impact your banking transactions differently than going from 3 children to 4.\n",
    "- Activation function is applied to a value coming into the node, to produce node output.\n",
    "- Traditionally, an S-shaped function **tanh** has been a popular choice for activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/forward_activation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Today the industry standard is **ReLU (Rectified Linear Unit) function.**\n",
    "- It has 2 linear units, but is surprisingly powerful when composed together over successful hidden layers.\n",
    "- It returns 0 for negative inputs, and same as input otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Networks\n",
    "- Till few years ago, 15 hidden layers were state of the art, today they can be scaled up to 1000s of layers.\n",
    "- The same forward propation process is iterated over subsequent layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/deeper_network.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These deep networks internally build **representations of patterns in the data** (that are useful for making predictions.\n",
    "- As a result they find increasingly complex patterns as we go through successive hidden layers of the network.\n",
    "- This way neural networks partially replace the need for feature engineering (manually creating better predictive features)\n",
    "- Deep Learning is also called **representation learning** because subsequent layers build increasingly sophisticated repsresentations of raw data, until we get to a stage where we can make predictions.\n",
    "- Eg. **Image Classigication**\n",
    "    <img src=\"files/image_classifier.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The earlier layers build patterns that are conceptually simple.\n",
    "- A simple interaction would look at groups of nearby pixels and find diagonal, horizontal, vertical lines, blurry areas etc.\n",
    "- Subsequent layers combine these informations to discoverlarger patterns like squares or other geometric shape.\n",
    "- Later layers then combine these shapes to identify face/car/etc.  \n",
    "  \n",
    "- Modeler doesn't need to specify those interactions, i.e. we do not tell the model to look for diagonal lines, etc.\n",
    "- When we train the model, the neural network gets weights  that find the relevant patterns to make better predictions.\n",
    "- This idea of finding increasingly complex/ abstract patterns is a recurring theme across domains/applications in DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a Neural Network with Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for Optimization\n",
    "- Merely having the structure of a neural network does not guarantee good predictive results.\n",
    "- We need to have a good choice of weights. If we change the weights, the output changes.\n",
    "- The error between predicted output and actual output is the loss.\n",
    "- For a single input, we may easily tweak the weights to get improved output.\n",
    "- But making accurate predictions gets **harder with multiple input points**.\n",
    "- For a given set of weights, there are many values of error - corresponding to many data point we make predictions on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "- A loss function is used to find a single value to evaluate model performance.\n",
    "- It aggregates all the errors in predictions from many data points into a single measure of model's predictive performance\n",
    "- A common loss function for regression task: mean squared error - square the difference, and take their **average** - a single score\n",
    "<img src=\"files/loss_function_gradient.png\">\n",
    "- Lower loss function value means a better model\n",
    "- **Goal:** Find weights that give lowest loss function value - We do this by **Gradient Descent** algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Analogy:\n",
    "1. Imagine you're in a pitch dark field, and want to find lowest point.\n",
    "2. Feel the ground to see how it slopes.\n",
    "3. Take a step downhill - an improvement, but not necessary the lowest point yet.\n",
    "4. Repeat until it is uphill in all directions.  \n",
    "This is roughly how gradient descent works.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the slope is positive:\n",
    "    * Going **opposite the slope** means going to a lower number.\n",
    "    * This can be achieved by subtracting the slope from current value of weights.\n",
    "    * But going too big a step may lead us astray.\n",
    "    * Solution: **Learning Rate**\n",
    "- **Learning Rate:**\n",
    "    - Instead of directly subtracting the slope, we multiply it by a small no. called learning rate before subtracting from the weight\n",
    "    - i.e Update each weight by subtracting **learning_rate * slope**\n",
    "- Learning rates are typically around **0.01**\n",
    "- This ensures we take smaller steps so that we can reliably move towards optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating slope\n",
    "- To calculate slope of loss for a  weight, we need to **multiply 3 things**:\n",
    "    1. Slope of loss function w.r.t value of node we feed into (target node)\n",
    "    2. Value of node that feeds into our weight (input node)\n",
    "    3. Slope of activation function w.r.t to value we feed into\n",
    "- i.e slope = 2 * error * input\n",
    "- Eg.\n",
    "<img src=\"files/weight_gradient_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. slope of loss function i.e MSE w.r.t prediction:  \n",
    "    * 2 * (Predicted value - Actual value) = 2 * Error\n",
    "    * 2 * (-4) = -8\n",
    "2. value of node we feed from = 3\n",
    "3. We don't have an activation function here, so we omit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So slop of loss = -8 * 3 = -24\n",
    "- Let learning rate be 0.01\n",
    "- So new weight = current_weight - 0.01(slope) = 2 - (-0.24) = **2.24**\n",
    "\n",
    "* For multiple weights, we repeat this process separately for each weight and simultaneously update them using their respective derivates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to calculate slopes and update multiple weights\n",
    "<img src=\"files/2_input_weight_update.png \"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.array([1, 2])\n",
    "input_data = np.array([3, 4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 40]\n"
     ]
    }
   ],
   "source": [
    "gradient = 2 * input_data * error\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "weights_updated = weights - learning_rate * gradient\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Updates to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def get_slope(input_data, target, weights):\n",
    "    predicted = (input_data * weights).sum()\n",
    "    error = (predicted - target)\n",
    "    slope = 2 * error * input_data\n",
    "    return slope\n",
    "\n",
    "def get_mse(input_data, target, weights):\n",
    "    predicted = (input_data * weights).sum()\n",
    "    return (predicted - target)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([1, 2, 3])\n",
    "weights = np.array([0,2,1])\n",
    "target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa16dd687f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = weights - 0.01 * slope\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "- Used to calculate slopes in complex deeper models\n",
    "- It allows gradient descent to update all weights in NN ( by  getting gradients or slopes of all the weights)\n",
    "- While forward propagation calculates node values and predictions using weights and inputs, backward propagation uses prediction error and calculates slopes sequentially from weights closest to output layer, through the hidden layers, back to the weights coming from inputs. \n",
    "- It comes from the chain rule of calculus.\n",
    "- Don't need to memorize mathematical details, important to understand the whole process\n",
    "<img src=\"files/backprop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation - Big Picture\n",
    "- We're trying to estimate the slope of loss function w.r.t each weight in our network (using prediction errors)\n",
    "- Do forward propagation first to calcuate predictions and error, and then use them to back propagate.\n",
    "- Go Back one layer at a time.\n",
    "- Gradients for weights is product of:\n",
    "    1. Node value feeding into that weight (input layer or hidden layers calculated during forward prop)\n",
    "    2. Slope of loss function w.r.t node  it feeds into (calculated in previous step of backprop)\n",
    "    3. Slope of activation function at weight's output node (For ReLU 0 or 1)\n",
    "- Need to keep track of slopes of loss function w.r.t node values - to calculate slopes of weights.\n",
    "- Slopes of node values are sum of the slopes of all weights coming out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
