{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Keras workflow\n",
    "1. Specify Architecture (*layers, nodes, activation functions, etc.*)\n",
    "2. Compile the model (*loss function, optimizer, etc.*)\n",
    "3. Fit (actual cycle of forward and back propagation)\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model\n",
    "- One of the 2 ways of building models in Keras, and the easier of the two.\n",
    "- Requires weights/connections only to 1 layer which is next in the network diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation Step\n",
    "\n",
    "1. Specify the optimizer\n",
    "    - It controls the learning rate\n",
    "    - The learning rate can greatly affect how quickly weights are computed and how good they are\n",
    "    - Many optimization algorithms themselves tune the learning rate.\n",
    "    - There are many options, each with it's own mathematical complexities.\n",
    "    - So it is good to follow a pragmatic approach of choosing 1 optimization algorithm and use it for most problems.\n",
    "    - 'Adam' is usually a good choice - It adjusts learning rate as it does gradient descent\n",
    "\n",
    "\n",
    "2. Loss function\n",
    "    - \"mean_squared_error\" is a common choice for regression problemms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "- Apply backpropagation and gradient descent with your data to update the weights\n",
    "- Scaling data before fitting further eases optimization, so that each feature on average is similar sized values\n",
    "    * One common technique is to subtract each feature by it's mean and divide by their standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "- Here the loss function to be used is **categorical_crossentropy**\n",
    "- Similar to logloss. The lower the better.\n",
    "- In the compile step, We add metrics=['accuracy'] to see performance of model at each step.\n",
    "- The output layer will now have multiple nodes, each corresponding to a possible outcome and will use softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Your Model - Save, Load, Predict\n",
    "- You can save the model by calling **.save()** method on model.\n",
    "- Models are saved in **HDF5** format for which **.h5** is the common extension\n",
    "- We can load the model by calling **load_model()** method from keras.models.\n",
    "- We can make predictions by calling predict() method and passing the data feature values, it will return the output in a same structure as target that we passed during training. It will list probabilities of each possible outcome.\n",
    "- We can verify a model structure after loading by calling **.summary()** method on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization - Choosing the right architecture and optimization arguments\n",
    "- Optimization - Hard problem\n",
    "    * An optimal value of a weight depends on other weights, and we update many weights simultaneously\n",
    "- We're simultaneously optimizing 1000s of parameters with complex relationships.\n",
    "- Even if the slope tells us which weights to increase and which ones to decrease, our model may not improve meaningfully.\n",
    "- A **small learning rate** causes to make such small updates to the weights that the model doesn't really improve materially.\n",
    "- A **large learning rate** may take us too far in the right direction.\n",
    "- Adam is a smart optimizer, but still there could be optimization problems\n",
    "- To understand the effect of learning rate, we can use SGD, where we try out different learning rates from a set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dying Neuron Problem\n",
    "- If *a neuron takes a value* less than 0\n",
    "- In ReLU function, a node with negative  input results in output 0, and the slope is also zero.\n",
    "- As a result any weights flowing into that node are also zero, hence those weights don't get updates.\n",
    "  \n",
    "- Once the node starts always getting negative inputs, It may continue getting negative inputs.\n",
    "- Hence it doesn't really contribute anything to the model, hence the name **\"Dead\"** neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we then use an activation function whose slope becomes exactly zero ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "- Occurs when many layers have very small slopes (eg. due to being on flat part of tanh curve)\n",
    "- Earlier activations like S-shaped tanh were used, whose slope outside the middle S was small.\n",
    "- In a deep network, repeated multiplication of small slopes cause slopes to get close to 0, and hence **updates to backprop were close to 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a phenomenon worth keeping in mind to understand why your model isn't training  better.  \n",
    "Changing the activation function may be the solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Typically a good model should show significantly improved loss in first few epochs and then rate of improvement slows down. \n",
    "If a model doesn't show improved loss in first few epochs, it could be due to:  \n",
    "- Too small learning rate\n",
    "- Too high learning rate\n",
    "- Poor choice of activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation in Deep Learning\n",
    "- Instead of relying on model performance on training date, we should validate it's performance on a held-out data.\n",
    "- Validation split is more commonly used than k-fold cross validation.\n",
    "- Ths is because deep learning is about large datasets, so computational expense of running k-fold calidation would be large.\n",
    "- Here we trust the single validation score because it is based on large amount of data, and is reliable.\n",
    "- Keras makes it easy for us to use some of our data for validation, by specifying it using **validation_split** in the fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping: Optimizing the optimization\n",
    "- We keep training the data as long as the validation score is improving. Once it stops improving, we stop training. This is Early Stopping.\n",
    "- We can use **EarlyStopping** from **keras.callbacks** to create an early stopping monitor, before the calling fit method. This monitor will check whether the validation score is improving in subsequent epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **patience** argument is used to specify the **number of epochs the model can go without improving**.  \n",
    "  **2 or 3** is a good choice. (Model may not improve after one epoch, but we should wait as it may improve in next epoch)\n",
    "- We then pass this monitor to the fit method under the argument **callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, validation_split = 0.3, callbacks = [early_stopping_monitor], epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We may later specify more callbacks in the list, when we've advanced our skills !)\n",
    "- Now that we have an early stopping callback, we can specify much higher max limit for number of epochs to run in **epochs** attribute.\n",
    "- Now our model will keep iterating until it doesn't improve before max limit, which is early stopping.  \n",
    "  This is a smarter training logic than relying on a fixed np. of epochs without looking at validation scores, while missing out on further possible improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Building great models requires experimentations:  \n",
    "- Experiment with different architectures\n",
    "- More layers\n",
    "- Fewer layers\n",
    "- Layers with more nodes\n",
    "- Layers with fewer nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
