{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Keras workflow\n",
    "1. Specify Architecture (*layers, nodes, activation functions, etc.*) **keras.model, model.add**\n",
    "2. Compile the model (*loss function, optimizer, etc.*) **model.compile**\n",
    "3. Fit (actual cycle of forward and back propagation) **model.fit**\n",
    "4. Predict **model.predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model\n",
    "- One of the 2 ways of building models in Keras, and the easier of the two.\n",
    "- Requires weights/connections only to 1 layer which is next in the network diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from tensorflow) (1.0.6)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/e0/d0/65fe48383146199f16dbd5999ef226b87bce63ad5cd73c840cf722637969/tensorboard-1.12.0-py3-none-any.whl\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from tensorflow) (1.0.5)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/f9/28787754923612ca9bfdffc588daa05580ed70698add063a5629d1a4209d/protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/4f/e9e84e4600c43cae7ce58489c6e73ff4c864557bc4d4d0f0029c79e07f31/grpcio-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.7MB 83kB/s eta 0:00:011   38% |████████████▎                   | 3.7MB 4.3MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/63/f505d2d4c21db849cf80bad517f0065a30be6b006b0a5637f1b95584a305/absl-py-0.6.1.tar.gz (94kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 1.1MB/s a 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: h5py in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ubuntu/miniconda/envs/av/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.0.0)\n",
      "Building wheels for collected packages: absl-py\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/18/ea/5e/e36e1b8739e78cd2eba0a08fdc602c2b16a4b263912af8cb64\n",
      "Successfully built absl-py\n",
      "Installing collected packages: astor, protobuf, werkzeug, markdown, grpcio, tensorboard, absl-py, gast, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.6.1 astor-0.7.1 gast-0.2.0 grpcio-1.16.1 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.0 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-0.14.1\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential() #model with 2 hidden layers and 1 output layer\n",
    "model.add(Dense(100, activation='relu', input_shape=(50,))) #nothing after comma indicates it can have any data points(rows)\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation Step\n",
    "\n",
    "1. Specify the **optimizer** - *sgd, rmsprop, etc.*\n",
    "    - It controls the learning rate.\n",
    "    - The learning rate can greatly affect how quickly weights are computed and how good they are.\n",
    "    - Many optimization algorithms themselves tune the learning rate.\n",
    "    - There are many options, each with it's own mathematical complexities.\n",
    "    - So it is good to follow a pragmatic approach of choosing 1 optimization algorithm and use it for most problems.\n",
    "    - **'Adam'** is usually a good choice - It adjusts learning rate as it does gradient descent\n",
    "\n",
    "\n",
    "2. **Loss function** - *binary_crossentropy, categorical_crossentropy, etc.*\n",
    "    - **\"mean_squared_error\"** is a common choice for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "- Apply backpropagation and gradient descent with your data to update the weights\n",
    "- Scaling data before fitting further eases optimization, so that each feature on average is similar sized values\n",
    "    * One common technique is to subtract each feature by it's mean and divide by their standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "- Here the loss function to be used is **categorical_crossentropy**\n",
    "- Similar to logloss. The lower the loss, the better.\n",
    "- In the compile step, We add **metrics=['accuracy']** to see performance of model at each step.\n",
    "- The output layer will now have multiple nodes, each corresponding to a possible outcome and will use softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Your Model - Save, Load, Predict\n",
    "- You can save the model by calling **.save()** method on model.\n",
    "- Models are saved in **HDF5** format for which **.h5** is the common extension\n",
    "- We can load the model by calling **load_model()** method from keras.models.\n",
    "- We can make predictions by calling predict() method and passing the data feature values, it will return the output in a same structure as target that we passed during training. It will list probabilities of each possible outcome.\n",
    "- We can verify a model structure after loading by calling **.summary()** method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('my_model.h5')\n",
    "model = load_model('my_model.h5')\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization - Choosing the right architecture and optimization arguments\n",
    "- Optimization - Hard problem\n",
    "    * An optimal value of a weight depends on other weights, and we update many weights simultaneously\n",
    "- We're simultaneously optimizing 1000s of parameters with complex relationships.\n",
    "- Even if the slope tells us which weights to increase and which ones to decrease, our model may not improve meaningfully.\n",
    "- A **small learning rate** causes to make such small updates to the weights that the model doesn't really improve materially.\n",
    "- A **large learning rate** may take us too far in the right direction.\n",
    "- Adam is a smart optimizer, but still there could be optimization problems\n",
    "- To understand the effect of learning rate, we can use **SGD** (Stochastic Gradient Descent), where we try out different learning rates from a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr = lr)\n",
    "    model.compile(optimizer=my_optimizer, loss= = 'categorical_entropy')\n",
    "    predictions = model.fit(predictors, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dying Neuron Problem\n",
    "<img src=\"files/dying_neuron_relu.png\">  \n",
    "- If *a neuron takes a value* less than 0  \n",
    "- In ReLU function, a node with negative  input results in **output 0**, and **the slope is also zero** (as shown in aboce fig,)  \n",
    "- As a result any **weights flowing into that node** are also zero, hence those weights don't get updated.  \n",
    "  \n",
    "- Once the node starts always getting negative inputs, It may continue getting negative inputs.  \n",
    "- Hence it doesn't really contribute anything to the model, hence the name **\"Dead\"** neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we then use an activation function whose slope never becomes exactly zero ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "- Occurs when many layers have very small slopes (eg. due to being on flat part of tanh curve)\n",
    "- Earlier, activations like S-shaped tanh were used, whose slope outside the middle S was small.\n",
    "- In a deep network, repeated multiplication of small slopes cause slopes to get close to 0, and hence **updates to backprop were close to 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a phenomenon *worth keeping in mind* to understand why your model isn't training  better.  \n",
    "**Changing the activation function may be the solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Typically a good model should show significantly improved loss in first few epochs and then rate of improvement slows down. \n",
    "If a model doesn't show improved loss in first few epochs, it could be due to:  \n",
    "- Too small learning rate\n",
    "- Too high learning rate\n",
    "- Poor choice of activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation in Deep Learning\n",
    "- Instead of relying on model performance on training data, we should validate it's performance on a held-out data.\n",
    "- Validation split is more commonly used than k-fold cross validation.\n",
    "- Ths is because deep learning is about large datasets, so computational expense of running k-fold validation would be large.\n",
    "- Here we trust the single validation score because it is based on large amount of data, and is reliable.\n",
    "- Keras makes it easy for us to use some of our data for validation, by specifying it using **validation_split** in the fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping: Optimizing the optimization\n",
    "- Stop optimization when it isn't helping any more.\n",
    "- We keep training the data as long as the validation score is improving. Once it stops improving, we stop training. This is Early Stopping.\n",
    "- We can use **EarlyStopping** from **keras.callbacks** to create an early stopping monitor, before calling the fit method. This monitor will check whether the validation score is improving in subsequent epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **patience** argument is used to specify the **number of epochs the model can go without improving**.  \n",
    "  **2 or 3** is a good choice. (Model may not improve after one epoch, but we should wait as it may improve in next epoch)\n",
    "- We then pass this monitor to the fit method under the argument **callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, validation_split = 0.3, callbacks = [early_stopping_monitor], epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We may later specify more callbacks in the list, when we've advanced our skills !)\n",
    "- Now that we have an early stopping callback, we can specify much higher max limit for number of epochs to run in **epochs** attribute.\n",
    "- Now our model will keep iterating until it doesn't improve before max limit, which is early stopping.  \n",
    "  This is a smarter training logic than relying on a fixed no. of epochs without looking at validation scores, while missing out on further possible improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Building great models requires experimentations:  \n",
    "- Experiment with different architectures\n",
    "- More layers\n",
    "- Fewer layers\n",
    "- Layers with more nodes\n",
    "- Layers with fewer nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Keras model by adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Capacity\n",
    "- There A little more art to finding good DL architectures than for fine-tuning other ML algos.\n",
    "- 'Model Capacity' should be considered while deciding what models to try.\n",
    "- **Model Capacity** is a model's ability to capture predictive patterns in your data. This is similar to concepts of overfitting underfitting.\n",
    "<img src=\"files/overfitting.png\">  \n",
    "- Overfitting is a model's ability to fit oddities in training data ( that are purely due to happenstance, and won't be present in new dataset)\n",
    "- In Underfitting, model fails to find important predictive patterns in training data.\n",
    "- The more the capacity of our Deep Learning Model, the further right we will be on the above graph (i.e more complex model)\n",
    "- Increasing number of layers or number of nodes per layer increases the model capacity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for optimizing model capacity\n",
    "1. Start with a simple network (baseline model)\n",
    "2. Get the validation score\n",
    "3. Keep increasing capacity as long as the validation score is improving\n",
    "4. Once it stops improving, we can decrease capacity slightly but that's still near the ideal  \n",
    "Here's a sequential experiment trying to optimize model capacity:  \n",
    "<img src=\"files/capacity_experiment.png\">  \n",
    "Should we change capacity by **adding nodes to existing layer** or **adding another layer** ?  \n",
    "No Universal Answer to that.  Keep experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL on Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing handwritten digits : MNIST\n",
    "- 28 x 28 pixels  grid flattened to 784  values for each image\n",
    "- Each value denotes darkness of that pixel\n",
    "- Create a DL model that takes those 784 features of each images as inputs, and predicts one of the 10 possible values for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0],-1)).astype('float32')/255\n",
    "X_test = X_test.reshape((X_test.shape[0],-1)).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical encoding of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model, Compile and Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 5s 112us/step - loss: 0.3430 - acc: 0.9030 - val_loss: 0.1988 - val_acc: 0.9423\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 4s 93us/step - loss: 0.1541 - acc: 0.9543 - val_loss: 0.1548 - val_acc: 0.9547\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.1134 - acc: 0.9657 - val_loss: 0.1314 - val_acc: 0.9605\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.0907 - acc: 0.9717 - val_loss: 0.1366 - val_acc: 0.9596\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.0759 - acc: 0.9767 - val_loss: 0.1129 - val_acc: 0.9658\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.0611 - acc: 0.9807 - val_loss: 0.1128 - val_acc: 0.9666\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.0526 - acc: 0.9828 - val_loss: 0.1181 - val_acc: 0.9663\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 4s 96us/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.1258 - val_acc: 0.9676\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 4s 95us/step - loss: 0.0401 - acc: 0.9866 - val_loss: 0.1199 - val_acc: 0.9677\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 4s 94us/step - loss: 0.0352 - acc: 0.9886 - val_loss: 0.1209 - val_acc: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb6c6ad128>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape = (784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu') )\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "'''# Create early stopping callback\n",
    "early_stop_monitor = EarlyStopping(patience=2)'''\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_split=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 5s 120us/step - loss: 0.3494 - acc: 0.8983 - val_loss: 0.1967 - val_acc: 0.9416\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.1621 - acc: 0.9512 - val_loss: 0.1762 - val_acc: 0.9489\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 4s 101us/step - loss: 0.1213 - acc: 0.9635 - val_loss: 0.1357 - val_acc: 0.9596\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 4s 103us/step - loss: 0.0993 - acc: 0.9701 - val_loss: 0.1567 - val_acc: 0.9546\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 4s 100us/step - loss: 0.0791 - acc: 0.9752 - val_loss: 0.1379 - val_acc: 0.9606\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.0697 - acc: 0.9779 - val_loss: 0.1319 - val_acc: 0.9628\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.0581 - acc: 0.9812 - val_loss: 0.1455 - val_acc: 0.9609\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 4s 102us/step - loss: 0.0535 - acc: 0.9825 - val_loss: 0.1425 - val_acc: 0.9630\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 4s 105us/step - loss: 0.0459 - acc: 0.9849 - val_loss: 0.1296 - val_acc: 0.9668\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 4s 104us/step - loss: 0.0400 - acc: 0.9870 - val_loss: 0.1456 - val_acc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb6c1f7860>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape = (784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu') )\n",
    "model.add(Dense(50, activation='relu') )\n",
    "\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stop_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_split=0.3, epochs=10, callbacks=[early_stop_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 43us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13199304647133686, 0.9672]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Start with standard prediction problems on tables of numbers (pandas or numpy arrays)\n",
    "- Images (with convolutional neural networks) are common next steps (or text or sound or something else !)\n",
    "- keras.io for excellent documentation\n",
    "- Graphical processing unit (GPU) provides dramatic speedups in model training times\n",
    "- Need a CUDA compatible GPU\n",
    "- For training on using **GPUs in the cloud** look here: http://bit.ly/2mYQXQb\n",
    "- Kaggle datasets and it's forum\n",
    "- [Wikipage](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research) on datasets\n",
    "- Keras, TF repo on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
